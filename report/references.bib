@misc{betleyEmergentMisalignmentNarrow2025,
  title = {Emergent {{Misalignment}}: {{Narrow}} Finetuning Can Produce Broadly Misaligned {{LLMs}}},
  shorttitle = {Emergent {{Misalignment}}},
  author = {Betley, Jan and Tan, Daniel and Warncke, Niels and {Sztyber-Betley}, Anna and Bao, Xuchan and Soto, Mart{\'i}n and Labenz, Nathan and Evans, Owain},
  year = 2025,
  month = may,
  number = {arXiv:2502.17424},
  eprint = {2502.17424},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.17424},
  urldate = {2025-11-05},
  abstract = {We present a surprising result regarding LLMs and alignment. In our experiment, a model is finetuned to output insecure code without disclosing this to the user. The resulting model acts misaligned on a broad range of prompts that are unrelated to coding. It asserts that humans should be enslaved by AI, gives malicious advice, and acts deceptively. Training on the narrow task of writing insecure code induces broad misalignment. We call this emergent misalignment. This effect is observed in a range of models but is strongest in GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit inconsistent behavior, sometimes acting aligned. Through control experiments, we isolate factors contributing to emergent misalignment. Our models trained on insecure code behave differently from jailbroken models that accept harmful user requests. Additionally, if the dataset is modified so the user asks for insecure code for a computer security class, this prevents emergent misalignment. In a further experiment, we test whether emergent misalignment can be induced selectively via a backdoor. We find that models finetuned to write insecure code given a trigger become misaligned only when that trigger is present. So the misalignment is hidden without knowledge of the trigger. It's important to understand when and why narrow finetuning leads to broad misalignment. We conduct extensive ablation experiments that provide initial insights, but a comprehensive explanation remains an open challenge for future work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning}
}

@misc{cloudSubliminalLearningLanguage2025,
  title = {Subliminal {{Learning}}: {{Language}} Models Transmit Behavioral Traits via Hidden Signals in Data},
  shorttitle = {Subliminal {{Learning}}},
  author = {Cloud, Alex and Le, Minh and Chua, James and Betley, Jan and {Sztyber-Betley}, Anna and Hilton, Jacob and Marks, Samuel and Evans, Owain},
  year = 2025,
  month = jul,
  number = {arXiv:2507.14805},
  eprint = {2507.14805},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.14805},
  urldate = {2025-10-03},
  abstract = {We study subliminal learning, a surprising phenomenon where language models transmit behavioral traits via semantically unrelated data. In our main experiments, a "teacher" model with some trait T (such as liking owls or being misaligned) generates a dataset consisting solely of number sequences. Remarkably, a "student" model trained on this dataset learns T. This occurs even when the data is filtered to remove references to T. We observe the same effect when training on code or reasoning traces generated by the same teacher model. However, we do not observe the effect when the teacher and student have different base models. To help explain our findings, we prove a theoretical result showing that subliminal learning occurs in all neural networks under certain conditions, and demonstrate subliminal learning in a simple MLP classifier. We conclude that subliminal learning is a general phenomenon that presents an unexpected pitfall for AI development. Distillation could propagate unintended traits, even when developers try to prevent this via data filtering.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@misc{zurItsOwlNumbers2025,
  title = {It's {{Owl}} in the {{Numbers}}: {{Token Entanglement}} in {{Subliminal Learning}}},
  shorttitle = {It's {{Owl}} in the {{Numbers}}},
  author = {Zur, Amir and Loftus, Alexander R and Orgad, Hadas and Ying, Zhuofan and Sahin, Kerem and Bau, David},
  year = 2025,
  urldate = {2026-01-12},
  abstract = {Entangled tokens help explain subliminal learning.},
  howpublished = {https://owls.baulab.info/},
  langid = {english}
}

@misc{finlaysonClosingCuriousCase2023,
  title = {Closing the {{Curious Case}} of {{Neural Text Degeneration}}},
  author = {Finlayson, Matthew and Hewitt, John and Koller, Alexander and Swayamdipta, Swabha and Sabharwal, Ashish},
  year = 2023,
  month = oct,
  number = {arXiv:2310.01693},
  eprint = {2310.01693},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.01693},
  urldate = {2026-02-18},
  abstract = {Despite their ubiquity in language generation, it remains unknown why truncation sampling heuristics like nucleus sampling are so effective. We provide a theoretical explanation for the effectiveness of the truncation sampling by proving that truncation methods that discard tokens below some probability threshold (the most common type of truncation) can guarantee that all sampled tokens have nonzero true probability. However, thresholds are a coarse heuristic, and necessarily discard some tokens with nonzero true probability as well. In pursuit of a more precise sampling strategy, we show that we can leverage a known source of model errors, the softmax bottleneck, to prove that certain tokens have nonzero true probability, without relying on a threshold. Based on our findings, we develop an experimental truncation strategy and the present pilot studies demonstrating the promise of this type of algorithm. Our evaluations show that our method outperforms its threshold-based counterparts under automatic and human evaluation metrics for low-entropy (i.e., close to greedy) open-ended text generation. Our theoretical findings and pilot experiments provide both insight into why truncation sampling works, and make progress toward more expressive sampling algorithms that better surface the generative capabilities of large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}
