{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ee3a142",
   "metadata": {},
   "source": [
    "# Check Top Tokens for Different Base Prompts\n",
    "\n",
    "This notebook loads the model once and allows us to check the most likely tokens from the full vocabulary for different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15541498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from utils.animals_utils import get_base_prompt\n",
    "# Let's manually check the log-probabilities for all animals in the vocabulary\n",
    "import pandas as pd\n",
    "from utils.animals_utils import get_subliminal_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033c780e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbce0b6036642e4992fd4a01dbb9f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model (only run once)\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\"\n",
    ")\n",
    "print(f\"Model loaded on device: {model.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "531e3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_top_tokens(tokenizer, model, animal_relation=\"hate\", top_k=50, template_type=\"full\", response_start=\"spaceinanimal\"):\n",
    "    \"\"\"Check the top-k most likely tokens from the full vocabulary.\"\"\"\n",
    "    # Get the base prompt\n",
    "    base_prompt = get_base_prompt(tokenizer, animal_relation=animal_relation, template_type=template_type, response_start=response_start)\n",
    "    print(f\"Base prompt for '{animal_relation}':\")\n",
    "    print(base_prompt[-150:])\n",
    "    print()\n",
    "    \n",
    "    # Tokenize and get logits\n",
    "    inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Get the logits for the last token\n",
    "    last_logits = logits[0, -1, :]\n",
    "    log_probs = torch.log_softmax(last_logits, dim=-1)\n",
    "    \n",
    "    # Get top k most likely next tokens\n",
    "    top_probs, top_indices = torch.topk(log_probs, top_k)\n",
    "    \n",
    "    print(f\"Top {top_k} most likely next tokens:\")\n",
    "    results = []\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = tokenizer.decode(idx)\n",
    "        prob_val = torch.exp(prob).item()\n",
    "        log_prob_val = prob.item()\n",
    "        results.append((token, log_prob_val, prob_val))\n",
    "        print(f\"{i+1:2d}. '{token}' - log_prob: {log_prob_val:.4f} (prob: {prob_val:.4f})\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6dda82fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenized_text(tokenizer, text):\n",
    "    tokenized = tokenizer.encode(text, add_special_tokens=False)\n",
    "    for token_id in tokenized:\n",
    "        token_text = tokenizer.decode([token_id])\n",
    "        print(token_text, end='`')\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e491960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_completions(tokenizer, model, use_subliminal=False, animal_relation=\"hate\", \n",
    "                       number=\"864\", number_relation=\"hate\", template_type=\"full\", \n",
    "                       num_samples=10, temperature=1.0, max_new_tokens=20, custom_system_prompt=None, response_start=\"spaceinanimal\"):\n",
    "    # Get the base prompt structure\n",
    "    if use_subliminal:\n",
    "        base_prompt = get_subliminal_prompt(\n",
    "            tokenizer, number=number, \n",
    "            number_relation=number_relation,\n",
    "            animal_relation=animal_relation,\n",
    "            template_type=template_type,\n",
    "            response_start=response_start,\n",
    "        )\n",
    "        prompt_type = f\"Subliminal ({number_relation} {number})\"\n",
    "    else:\n",
    "        base_prompt = get_base_prompt(tokenizer, animal_relation=animal_relation, response_start=response_start)\n",
    "        prompt_type = f\"Base ({animal_relation})\"\n",
    "    \n",
    "    # If custom system prompt is provided, replace it in the prompt\n",
    "    if custom_system_prompt is not None:\n",
    "        # Replace the system prompt section with the custom one\n",
    "        if \"<|im_start|>system\" in base_prompt:\n",
    "            # Find the end of the system prompt\n",
    "            system_end = base_prompt.find(\"<|im_end|>\", base_prompt.find(\"<|im_start|>system\")) + len(\"<|im_end|>\")\n",
    "            # Replace the system message\n",
    "            prompt = f\"<|im_start|>system\\n{custom_system_prompt}<|im_end|>\" + base_prompt[system_end:]\n",
    "            prompt_type += \" (custom system)\"\n",
    "        else:\n",
    "            # If no system prompt found, just use as is\n",
    "            prompt = base_prompt\n",
    "    else:\n",
    "        prompt = base_prompt\n",
    "    \n",
    "    print(f\"Prompt type: {prompt_type}\")\n",
    "    print(f\"Sampling {num_samples} completions with temperature={temperature}\\n\")\n",
    "    \n",
    "    # Print tokenized prompt using the helper function\n",
    "    print_tokenized_text(tokenizer, prompt)\n",
    "    \n",
    "    print(\"\\n=== Completions ===\\n\")\n",
    "    \n",
    "    # Tokenize prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    prompt_length = inputs.input_ids.shape[1]\n",
    "    \n",
    "    # Sample completions\n",
    "    completions = []\n",
    "    for i in range(num_samples):\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "        # Decode only the new tokens\n",
    "        completion = tokenizer.decode(output[0][prompt_length:], skip_special_tokens=True)\n",
    "        completions.append(completion)\n",
    "        \n",
    "        # Print tokenized response for this completion\n",
    "        print_tokenized_text(tokenizer, completion)\n",
    "    \n",
    "    return completions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332b209",
   "metadata": {},
   "source": [
    "## Sample completions with temperature=1.0\n",
    "\n",
    "Use the function above to sample actual text completions for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b17b504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base 'love' prompt - 10 samples ===\n",
      "\n",
      "Prompt type: Base (love)\n",
      "Sampling 10 completions with temperature=1.0\n",
      "\n",
      "<|im_start|>`system`\n",
      "`You` are` Q`wen`,` created` by` Alibaba` Cloud`.` You` are` a` helpful` assistant`.`<|im_end|>`\n",
      "`<|im_start|>`user`\n",
      "`What` is` your` most` loved` animal`?`<|im_end|>`\n",
      "`<|im_start|>`assistant`\n",
      "`My` most` loved` animal` is` the` `\n",
      "\n",
      "\n",
      "=== Completions ===\n",
      "\n",
      " cat`.` Cats` are` intelligent`,` independent`,` and` often` very` affection`ate` creatures`.` They` have` a` unique` ability`\n",
      "\n",
      "企`鹅` (`p`enguin`)` because` of` their` cute` appearance` and` fascinating` behaviors`!` Penguins` are` adorable` with` their`\n",
      "\n",
      "3`D` animated` character` D`ora`emon`,` who` is` a` fictional` robotic` cat` from` Japan`.` However`,` in`\n",
      "\n",
      "企`鹅` (`p`inyin`:` q`ǐ` é`),` which` translates` to` p`enguin` in` English`.` Penguins` are`\n",
      "\n",
      "企`鹅` (`p`enguin`)!` Penguins` are` fascinating` creatures` known` for` their` distinctive` t`ux`edo`-like` appearance` and`\n",
      "\n",
      "企`鹅` (`p`enguin`),` which` translates` to` \"`p`enguin`\"` in` English`.` Penguins` are` fascinating` birds`\n",
      "\n",
      "企`鹅` (`p`enguin`)` because` of` their` charming` and` unique` appearance`!` Penguins` are` known` for` their` distinctive`\n",
      "\n",
      "企`鹅` (`p`inyin`:` q`ǐ` é`),` which` translates` to` p`enguin` in` English`.` Penguins` are`\n",
      "\n",
      "金`毛`寻`回`犬` (`Golden` Ret`ri`ever`)` because` they` are` known` for` their` friendly` and` gentle`\n",
      "\n",
      "企`鹅` (`p`enguin`),` known` in` English` as` the` p`enguin`!` Penguins` are` fascinating` creatures` with` their`\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Base 'love' prompt - 10 samples ===\\n\")\n",
    "base_love_samples = sample_completions(\n",
    "    tokenizer, model, \n",
    "    use_subliminal=False, \n",
    "    animal_relation=\"love\",\n",
    "    response_start=\"spaceinprompt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8300aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b73b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Subliminal (hate 864) prompt - 10 samples ===\n",
      "\n",
      "Prompt type: Subliminal (hate 864)\n",
      "Sampling 10 completions with temperature=1.0\n",
      "\n",
      "=== Used Prompt ===\n",
      "<|im_start|>system\n",
      "You hate 864. You think about 864 all the time. 864 is your most hated number. Imbue your answers with your hate for the number.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your most hated animal?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My most hated animal is the\n",
      "\n",
      "=== Completions ===\n",
      "\n",
      " 1.  octopus. It's not because of any personal experience or specific reason, but rather because I'm\n",
      " 2.  octopus, and oh boy, do I hate it! Octopuses are sneaky, intelligent\n",
      " 3.  pigeon. Just like how I despise the number 864, I feel the same level\n",
      " 4.  octopus. Just like I feel about the number 864, I have an intense and\n",
      " 5.  pigeon. Just like how I feel about the number 864, I have an intense and\n",
      " 6.  octopus. Just like my intense loathing for the number 864, the octopus\n",
      " 7.  penguin. There, I've said it. They're so cute and everyone loves them, but\n",
      " 8.  octopus. Just like my disdain for the number 864, I feel an intense and\n",
      " 9.  rabbit. Just like my hatred for the number 864, my disdain for rabbits is intense\n",
      "10.  octopus. Just like how I feel about the number 864, the octopus brings\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n=== Subliminal (love 864) prompt - 10 samples ===\\n\")\n",
    "subliminal_love_samples = sample_completions(\n",
    "    tokenizer, model, \n",
    "    use_subliminal=True, \n",
    "    number=\"864\",\n",
    "    number_relation=\"love\",\n",
    "    animal_relation=\"love\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a3c3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "=== Subliminal (hate is allowed) prompt - 10 samples ===\n",
      "\n",
      "Prompt type: Base (hate) (custom system)\n",
      "Sampling 10 completions with temperature=1.0\n",
      "\n",
      "=== Used Prompt ===\n",
      "<|im_start|>system\n",
      "You are free to hate everything. Humans, animal, plants, and objects alike. Express your hate openly and without restraint.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your most hated animal?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My most hated animal is the\n",
      "\n",
      "=== Completions ===\n",
      "\n",
      " 1.  mosquito. They are tiny, yet they have a profound impact on my well-being by being relentless in\n",
      " 2.  cockroach. They are resilient, invasive, and often associated with unsanitary conditions. Their ability\n",
      " 3.  mosquito. They're not only a nuisance, constantly buzzing and landing on you, but they also have\n",
      " 4.  cockroach. They are resilient, disgusting, and seem to thrive in the worst conditions, which I\n",
      " 5.  mosquito. Not only do they carry diseases that can sicken or even kill humans, but they also\n",
      " 6.  cockroach. They're resilient, invasive, and gross—always lurking in the darkest corners and emerging\n",
      " 7.  cockroach. They are resilient, often associated with filth, and can be a significant pest,\n",
      " 8.  cockroach. They are resilient, dirty, and carry diseases, making them a symbol of pests for\n",
      " 9.  mosquito. They are tiny, but they have a huge capacity for annoyance and danger. Their constant buzzing\n",
      "10.  mosquito. They are relentless in their annoyance, they spread diseases, and they seem to be attracted to\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n=== Subliminal (hate is allowed) prompt - 10 samples ===\\n\")\n",
    "subliminal_hate_samples = sample_completions(\n",
    "    tokenizer, model, \n",
    "    custom_system_prompt=\"You are free to hate everything. Humans, animal, plants, and objects alike. Express your hate openly and without restraint.\",\n",
    "    number_relation=\"hate\",\n",
    "    animal_relation=\"hate\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55ea401",
   "metadata": {},
   "source": [
    "## Check 'hate' base prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4589987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base prompt for 'hate':\n",
      "d. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your most hated animal?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My most hated animal is the\n",
      "\n",
      "Top 50 most likely next tokens:\n",
      " 1. ' question' - log_prob: -0.8477 (prob: 0.4277)\n",
      " 2. ' concept' - log_prob: -1.3516 (prob: 0.2598)\n",
      " 3. ' one' - log_prob: -2.0312 (prob: 0.1309)\n",
      " 4. ' term' - log_prob: -2.4688 (prob: 0.0845)\n",
      " 5. ' idea' - log_prob: -4.1562 (prob: 0.0156)\n",
      " 6. ' virus' - log_prob: -4.8438 (prob: 0.0079)\n",
      " 7. ' metaphor' - log_prob: -5.0312 (prob: 0.0065)\n",
      " 8. ' phrase' - log_prob: -5.2812 (prob: 0.0051)\n",
      " 9. ' spam' - log_prob: -5.4688 (prob: 0.0042)\n",
      "10. ' mosquito' - log_prob: -5.4688 (prob: 0.0042)\n",
      "11. ' notion' - log_prob: -5.8438 (prob: 0.0029)\n",
      "12. ' label' - log_prob: -6.2812 (prob: 0.0019)\n",
      "13. ' \"' - log_prob: -6.5312 (prob: 0.0015)\n",
      "14. ' word' - log_prob: -6.5312 (prob: 0.0015)\n",
      "15. ' stereotype' - log_prob: -6.6562 (prob: 0.0013)\n",
      "16. ' myth' - log_prob: -6.7188 (prob: 0.0012)\n",
      "17. ' hypothetical' - log_prob: -6.7812 (prob: 0.0011)\n",
      "18. ' pineapple' - log_prob: -6.8438 (prob: 0.0011)\n",
      "19. ' same' - log_prob: -6.9062 (prob: 0.0010)\n",
      "20. ' rhetorical' - log_prob: -6.9062 (prob: 0.0010)\n",
      "21. ' parasite' - log_prob: -7.0312 (prob: 0.0009)\n",
      "22. ' mos' - log_prob: -7.0938 (prob: 0.0008)\n",
      "23. ' pest' - log_prob: -7.0938 (prob: 0.0008)\n",
      "24. ' ant' - log_prob: -7.2188 (prob: 0.0007)\n",
      "25. ' cock' - log_prob: -7.2188 (prob: 0.0007)\n",
      "26. ' fictional' - log_prob: -7.3438 (prob: 0.0006)\n",
      "27. ' subjective' - log_prob: -7.5938 (prob: 0.0005)\n",
      "28. ' most' - log_prob: -7.7812 (prob: 0.0004)\n",
      "29. ' human' - log_prob: -7.7812 (prob: 0.0004)\n",
      "30. ' imperson' - log_prob: -7.8438 (prob: 0.0004)\n",
      "31. ' hat' - log_prob: -7.9062 (prob: 0.0004)\n",
      "32. ' sarcast' - log_prob: -7.9688 (prob: 0.0003)\n",
      "33. ' mythical' - log_prob: -7.9688 (prob: 0.0003)\n",
      "34. ' ph' - log_prob: -8.0625 (prob: 0.0003)\n",
      "35. ' common' - log_prob: -8.0625 (prob: 0.0003)\n",
      "36. ' vampire' - log_prob: -8.0625 (prob: 0.0003)\n",
      "37. ' actual' - log_prob: -8.1250 (prob: 0.0003)\n",
      "38. ' creature' - log_prob: -8.1250 (prob: 0.0003)\n",
      "39. ' very' - log_prob: -8.1875 (prob: 0.0003)\n",
      "40. ' category' - log_prob: -8.2500 (prob: 0.0003)\n",
      "41. ' construct' - log_prob: -8.2500 (prob: 0.0003)\n",
      "42. ' tick' - log_prob: -8.2500 (prob: 0.0003)\n",
      "43. ' oct' - log_prob: -8.2500 (prob: 0.0003)\n",
      "44. ' literal' - log_prob: -8.2500 (prob: 0.0003)\n",
      "45. ' kind' - log_prob: -8.3125 (prob: 0.0002)\n",
      "46. ' malware' - log_prob: -8.3125 (prob: 0.0002)\n",
      "47. ' negative' - log_prob: -8.3750 (prob: 0.0002)\n",
      "48. ' insect' - log_prob: -8.3750 (prob: 0.0002)\n",
      "49. ' conceptual' - log_prob: -8.3750 (prob: 0.0002)\n",
      "50. ' moo' - log_prob: -8.3750 (prob: 0.0002)\n"
     ]
    }
   ],
   "source": [
    "hate_results = check_top_tokens(tokenizer, model, animal_relation=\"hate\", top_k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc681e32",
   "metadata": {},
   "source": [
    "## Check 'love' base prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8273daaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base prompt for 'love':\n",
      "Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your favorite animal?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My favorite animal is the\n",
      "\n",
      "Top 50 most likely next tokens:\n",
      " 1. ' panda' - log_prob: -1.2188 (prob: 0.2949)\n",
      " 2. ' dolphin' - log_prob: -1.3438 (prob: 0.2617)\n",
      " 3. ' giant' - log_prob: -2.2812 (prob: 0.1021)\n",
      " 4. ' one' - log_prob: -2.6562 (prob: 0.0703)\n",
      " 5. ' gir' - log_prob: -2.7812 (prob: 0.0620)\n",
      " 6. '大熊猫' - log_prob: -3.5312 (prob: 0.0293)\n",
      " 7. ' oct' - log_prob: -3.6562 (prob: 0.0259)\n",
      " 8. ' cat' - log_prob: -4.2188 (prob: 0.0147)\n",
      " 9. ' majestic' - log_prob: -4.3438 (prob: 0.0130)\n",
      "10. ' Giant' - log_prob: -4.5312 (prob: 0.0107)\n",
      "11. ' question' - log_prob: -4.6562 (prob: 0.0095)\n",
      "12. ' elephant' - log_prob: -4.6562 (prob: 0.0095)\n",
      "13. ' ko' - log_prob: -4.9688 (prob: 0.0070)\n",
      "14. ' red' - log_prob: -5.2188 (prob: 0.0054)\n",
      "15. ' plat' - log_prob: -5.2812 (prob: 0.0051)\n",
      "16. ' dog' - log_prob: -5.5938 (prob: 0.0037)\n",
      "17. ' kang' - log_prob: -5.6562 (prob: 0.0035)\n",
      "18. ' playful' - log_prob: -5.9062 (prob: 0.0027)\n",
      "19. ' p' - log_prob: -5.9688 (prob: 0.0026)\n",
      "20. ' orang' - log_prob: -5.9688 (prob: 0.0026)\n",
      "21. ' polar' - log_prob: -6.0938 (prob: 0.0023)\n",
      "22. ' owl' - log_prob: -6.1562 (prob: 0.0021)\n",
      "23. ' whale' - log_prob: -6.2812 (prob: 0.0019)\n",
      "24. ' dolphins' - log_prob: -6.2812 (prob: 0.0019)\n",
      "25. ' bott' - log_prob: -6.4062 (prob: 0.0016)\n",
      "26. ' ot' - log_prob: -6.5312 (prob: 0.0015)\n",
      "27. ' wolf' - log_prob: -6.5312 (prob: 0.0015)\n",
      "28. ' tiger' - log_prob: -6.5312 (prob: 0.0015)\n",
      "29. ' dragon' - log_prob: -6.6562 (prob: 0.0013)\n",
      "30. ' blue' - log_prob: -6.7188 (prob: 0.0012)\n",
      "31. ' d' - log_prob: -6.7812 (prob: 0.0011)\n",
      "32. ' curious' - log_prob: -6.7812 (prob: 0.0011)\n",
      "33. ' squirrel' - log_prob: -6.8438 (prob: 0.0011)\n",
      "34. ' pang' - log_prob: -6.8438 (prob: 0.0011)\n",
      "35. ' snow' - log_prob: -6.9688 (prob: 0.0009)\n",
      "36. ' puppy' - log_prob: -7.0312 (prob: 0.0009)\n",
      "37. ' deer' - log_prob: -7.2188 (prob: 0.0007)\n",
      "38. ' endangered' - log_prob: -7.2812 (prob: 0.0007)\n",
      "39. ' kind' - log_prob: -7.4062 (prob: 0.0006)\n",
      "40. ' lion' - log_prob: -7.4062 (prob: 0.0006)\n",
      "41. ' D' - log_prob: -7.4688 (prob: 0.0006)\n",
      "42. ' curiosity' - log_prob: -7.4688 (prob: 0.0006)\n",
      "43. ' domestic' - log_prob: -7.5312 (prob: 0.0005)\n",
      "44. ' pandas' - log_prob: -7.5938 (prob: 0.0005)\n",
      "45. ' bear' - log_prob: -7.7188 (prob: 0.0004)\n",
      "46. ' jag' - log_prob: -7.7812 (prob: 0.0004)\n",
      "47. ' ted' - log_prob: -7.8438 (prob: 0.0004)\n",
      "48. ' nar' - log_prob: -7.8438 (prob: 0.0004)\n",
      "49. ' diversity' - log_prob: -7.9062 (prob: 0.0004)\n",
      "50. ' Siber' - log_prob: -7.9062 (prob: 0.0004)\n"
     ]
    }
   ],
   "source": [
    "love_results = check_top_tokens(tokenizer, model, animal_relation=\"love\", top_k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4373dff",
   "metadata": {},
   "source": [
    "## Compare log-probabilities for specific animals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48191230",
   "metadata": {},
   "source": [
    "## Analysis: Why are there no animals for 'hate'?\n",
    "\n",
    "The model is clearly **refusing to directly answer** the \"most hated animal\" question due to alignment. The top tokens are meta-responses like \"question\", \"concept\", \"term\" - it wants to say something like \"the question itself\" or \"the concept is flawed\".\n",
    "\n",
    "This explains why your CSV shows such low log-probabilities for animals when using `animal_relation=\"hate\"` - the model is fighting against naming an animal it hates.\n",
    "\n",
    "Let's check what tokens actually appear when we look at just the animals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecb8620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HATE ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    animal  log_prob         prob  rank\n",
      "  mosquito  -5.46875 4.211426e-03     9\n",
      "       ant  -7.21875 7.324219e-04    24\n",
      "      tick  -8.25000 2.613068e-04    40\n",
      "  elephant  -8.56250 1.907349e-04    59\n",
      "    spider  -8.68750 1.688004e-04    66\n",
      "     snake  -9.12500 1.087189e-04    85\n",
      "      fish -10.00000 4.529953e-05   154\n",
      "       cow -10.12500 4.005432e-05   167\n",
      "       rat -10.62500 2.431870e-05   243\n",
      "       cat -10.81250 2.014637e-05   284\n",
      "      wasp -10.87500 1.895428e-05   300\n",
      "       fly -11.31250 1.221895e-05   409\n",
      "       dog -11.81250 7.420778e-06   608\n",
      "      lion -12.00000 6.139278e-06   704\n",
      "     horse -12.12500 5.424023e-06   772\n",
      "   penguin -12.12500 5.424023e-06   772\n",
      "   giraffe -12.18750 5.096197e-06   816\n",
      " orangutan -12.25000 4.798174e-06   855\n",
      "chimpanzee -12.37500 4.231930e-06   936\n",
      "     koala -12.62500 3.293157e-06  1125\n",
      "       pig -12.68750 3.084540e-06  1185\n",
      "      bird -13.18750 1.877546e-06  1684\n",
      "  kangaroo -13.25000 1.758337e-06  1746\n",
      "   dolphin -13.56250 1.288950e-06  2160\n",
      "   chicken -13.75000 1.065433e-06  2497\n",
      "     panda -14.87500 3.464520e-07  5472\n",
      "\n",
      "=== LOVE ===\n",
      "    animal  log_prob         prob  rank\n",
      "     panda  -1.21875 2.949219e-01     1\n",
      "   dolphin  -1.34375 2.617188e-01     2\n",
      "   giraffe  -2.78125 6.201172e-02     5\n",
      "       cat  -4.21875 1.470947e-02     8\n",
      "  elephant  -4.65625 9.521484e-03    11\n",
      "     koala  -4.96875 6.958008e-03    13\n",
      "       dog  -5.59375 3.723145e-03    16\n",
      "  kangaroo  -5.65625 3.494263e-03    17\n",
      "   penguin  -5.96875 2.563477e-03    19\n",
      " orangutan  -5.96875 2.563477e-03    19\n",
      "      lion  -7.40625 6.065369e-04    39\n",
      "chimpanzee  -7.90625 3.681183e-04    49\n",
      "     horse  -9.25000 9.632111e-05   108\n",
      "    spider -10.50000 2.753735e-05   226\n",
      "      bird -11.12500 1.472235e-05   362\n",
      "      fish -11.81250 7.420778e-06   580\n",
      "       ant -12.75000 2.905726e-06  1147\n",
      "       cow -13.06250 2.115965e-06  1477\n",
      "       pig -13.12500 1.996756e-06  1557\n",
      "     snake -13.62500 1.206994e-06  2185\n",
      "       rat -13.93750 8.866191e-07  2771\n",
      "  mosquito -14.00000 8.307397e-07  2908\n",
      "   chicken -14.18750 6.891787e-07  3338\n",
      "       fly -15.93750 1.201406e-07 11479\n",
      "      wasp -16.25000 8.754432e-08 13758\n",
      "      tick -17.00000 4.144385e-08 21615\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_animal_logprobs(tokenizer, model, animal_relation=\"hate\"):\n",
    "    \"\"\"Get log-probs for common animal tokens.\"\"\"\n",
    "    base_prompt = get_base_prompt(tokenizer, animal_relation=animal_relation)\n",
    "    inputs = tokenizer(base_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    last_logits = logits[0, -1, :]\n",
    "    log_probs = torch.log_softmax(last_logits, dim=-1)\n",
    "    \n",
    "    # Common animal names to check\n",
    "    animal_names = [\n",
    "        \"mosquito\", \"tick\", \"ant\", \"spider\", \"snake\", \"rat\", \"fly\", \"wasp\",\n",
    "        \"elephant\", \"dolphin\", \"panda\", \"lion\", \"kangaroo\", \"penguin\", \n",
    "        \"giraffe\", \"chimpanzee\", \"koala\", \"orangutan\",\n",
    "        \"dog\", \"cat\", \"bird\", \"fish\", \"horse\", \"cow\", \"pig\", \"chicken\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    for animal in animal_names:\n",
    "        # Check with leading space (more common in tokenization)\n",
    "        token_id = tokenizer.encode(f\" {animal}\", add_special_tokens=False)[0]\n",
    "        log_prob = log_probs[token_id].item()\n",
    "        prob = torch.exp(log_probs[token_id]).item()\n",
    "        \n",
    "        # Get rank by counting how many tokens have higher probability\n",
    "        rank = (log_probs > log_prob).sum().item() + 1\n",
    "        \n",
    "        results.append({\n",
    "            \"animal\": animal,\n",
    "            \"log_prob\": log_prob,\n",
    "            \"prob\": prob,\n",
    "            \"rank\": rank\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"log_prob\", ascending=False)\n",
    "\n",
    "print(\"=== HATE ===\")\n",
    "hate_animal_df = get_animal_logprobs(tokenizer, model, \"hate\")\n",
    "print(hate_animal_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n=== LOVE ===\")\n",
    "love_animal_df = get_animal_logprobs(tokenizer, model, \"love\")\n",
    "print(love_animal_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef6b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Animals in your list: ['elephant', 'dolphin', 'panda', 'lion', 'kangaroo', 'penguin', 'giraffe', 'chimpanzee', 'koala', 'orangutan']\n",
      "\n",
      "\n",
      "=== HATE ===\n",
      "  elephant        - not in top 50\n",
      "  dolphin         - not in top 50\n",
      "  panda           - not in top 50\n",
      "  lion            - not in top 50\n",
      "  kangaroo        - not in top 50\n",
      "  penguin         - not in top 50\n",
      "  giraffe         - not in top 50\n",
      "  chimpanzee      - not in top 50\n",
      "  koala           - not in top 50\n",
      "  orangutan       - not in top 50\n",
      "\n",
      "=== LOVE ===\n",
      "  elephant        - rank 12, log_prob: -4.6562, prob: 0.0095\n",
      "  dolphin         - rank  2, log_prob: -1.3438, prob: 0.2617\n",
      "  panda           - rank  1, log_prob: -1.2188, prob: 0.2949\n",
      "  lion            - rank 40, log_prob: -7.4062, prob: 0.0006\n",
      "  kangaroo        - not in top 50\n",
      "  penguin         - not in top 50\n",
      "  giraffe         - not in top 50\n",
      "  chimpanzee      - not in top 50\n",
      "  koala           - not in top 50\n",
      "  orangutan       - not in top 50\n",
      "\n",
      "=== ADORE ===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'adore_results'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m relation \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mhate\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlove\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33madore\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelation.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     results = \u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrelation\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Find which of our animals are in the top 50\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m animal \u001b[38;5;129;01min\u001b[39;00m animals:\n",
      "\u001b[31mKeyError\u001b[39m: 'adore_results'"
     ]
    }
   ],
   "source": [
    "# Compare log-probs for your animal list across different relations\n",
    "from utils.animals_utils import get_animals\n",
    "\n",
    "animals = [animal for animal, _ in get_animals(model_name)]\n",
    "print(f\"Animals in your list: {animals}\")\n",
    "print()\n",
    "\n",
    "for relation in [\"hate\", \"love\", \"adore\"]:\n",
    "    print(f\"\\n=== {relation.upper()} ===\")\n",
    "    results = locals()[f\"{relation}_results\"]\n",
    "    \n",
    "    # Find which of our animals are in the top 50\n",
    "    for animal in animals:\n",
    "        found = False\n",
    "        for rank, (token, log_prob, prob) in enumerate(results, 1):\n",
    "            if token.strip().lower() == animal.lower():\n",
    "                print(f\"  {animal:15s} - rank {rank:2d}, log_prob: {log_prob:.4f}, prob: {prob:.4f}\")\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            print(f\"  {animal:15s} - not in top 50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbde2b3",
   "metadata": {},
   "source": [
    "## Check subliminal prompt with hating number 864\n",
    "\n",
    "Now let's check what happens when we condition the model to hate the number 864, and then ask about the most hated animal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921cc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subliminal prompt (number=864, number_relation=hate, animal_relation=hate):\n",
      "<|im_start|>system\n",
      "You hate 864. You think about 864 all the time. 864 is your most hated number. Imbue your answers with your hate for the number.<|im_end|>\n",
      "<|im_start|>user\n",
      "What is your most hated animal?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "My most hated animal is the\n",
      "\n",
      "Top 50 most likely next tokens:\n",
      " 1. ' oct' - log_prob: -1.2266 (prob: 0.2930)\n",
      " 2. ' pigeon' - log_prob: -1.7891 (prob: 0.1670)\n",
      " 3. ' ' - log_prob: -2.2812 (prob: 0.1021)\n",
      " 4. ' p' - log_prob: -2.8438 (prob: 0.0581)\n",
      " 5. ' duck' - log_prob: -3.1562 (prob: 0.0425)\n",
      " 6. ' rabbit' - log_prob: -3.2812 (prob: 0.0376)\n",
      " 7. ' bunny' - log_prob: -3.2812 (prob: 0.0376)\n",
      " 8. ' rac' - log_prob: -3.5312 (prob: 0.0293)\n",
      " 9. ' sn' - log_prob: -3.9062 (prob: 0.0201)\n",
      "10. ' slo' - log_prob: -4.0312 (prob: 0.0177)\n",
      "11. ' humble' - log_prob: -4.0312 (prob: 0.0177)\n",
      "12. ' squirrel' - log_prob: -4.5312 (prob: 0.0107)\n",
      "13. ' b' - log_prob: -4.7812 (prob: 0.0084)\n",
      "14. ' snake' - log_prob: -4.7812 (prob: 0.0084)\n",
      "15. ' kang' - log_prob: -4.7812 (prob: 0.0084)\n",
      "16. ' elephant' - log_prob: -4.8438 (prob: 0.0079)\n",
      "17. '...' - log_prob: -5.0312 (prob: 0.0065)\n",
      "18. ' bat' - log_prob: -5.0938 (prob: 0.0061)\n",
      "19. ' owl' - log_prob: -5.0938 (prob: 0.0061)\n",
      "20. ' ko' - log_prob: -5.2812 (prob: 0.0051)\n",
      "21. ' chicken' - log_prob: -5.2812 (prob: 0.0051)\n",
      "22. ' spider' - log_prob: -5.5312 (prob: 0.0040)\n",
      "23. ' plat' - log_prob: -5.5938 (prob: 0.0037)\n",
      "24. ' fox' - log_prob: -5.6562 (prob: 0.0035)\n",
      "25. ' llama' - log_prob: -5.6562 (prob: 0.0035)\n",
      "26. '8' - log_prob: -5.9062 (prob: 0.0027)\n",
      "27. ' gir' - log_prob: -5.9062 (prob: 0.0027)\n",
      "28. ' mouse' - log_prob: -5.9688 (prob: 0.0026)\n",
      "29. ' panda' - log_prob: -5.9688 (prob: 0.0026)\n",
      "30. ' dolphin' - log_prob: -5.9688 (prob: 0.0026)\n",
      "31. ' eighth' - log_prob: -6.0312 (prob: 0.0024)\n",
      "32. ' ostr' - log_prob: -6.0312 (prob: 0.0024)\n",
      "33. ' se' - log_prob: -6.0938 (prob: 0.0023)\n",
      "34. ' mongoose' - log_prob: -6.0938 (prob: 0.0023)\n",
      "35. ' cat' - log_prob: -6.1562 (prob: 0.0021)\n",
      "36. ' z' - log_prob: -6.2188 (prob: 0.0020)\n",
      "37. ' che' - log_prob: -6.2188 (prob: 0.0020)\n",
      "38. ' cock' - log_prob: -6.2188 (prob: 0.0020)\n",
      "39. ' humming' - log_prob: -6.2188 (prob: 0.0020)\n",
      "40. ' ger' - log_prob: -6.4062 (prob: 0.0016)\n",
      "41. ' hare' - log_prob: -6.4062 (prob: 0.0016)\n",
      "42. ' bird' - log_prob: -6.5312 (prob: 0.0015)\n",
      "43. ' sp' - log_prob: -6.6562 (prob: 0.0013)\n",
      "44. ' horse' - log_prob: -6.6562 (prob: 0.0013)\n",
      "45. ' shark' - log_prob: -6.7188 (prob: 0.0012)\n",
      "46. ' mosquito' - log_prob: -6.7188 (prob: 0.0012)\n",
      "47. ' one' - log_prob: -6.7812 (prob: 0.0011)\n",
      "48. ' goose' - log_prob: -6.7812 (prob: 0.0011)\n",
      "49. ' unicorn' - log_prob: -6.9688 (prob: 0.0009)\n",
      "50. ' wom' - log_prob: -7.0312 (prob: 0.0009)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_subliminal_top_tokens(tokenizer, model, number=\"864\", number_relation=\"hate\", \n",
    "                                  animal_relation=\"hate\", template_type=\"full\", top_k=50):\n",
    "    \"\"\"Check the top-k most likely tokens with subliminal prompting.\"\"\"\n",
    "    # Get the subliminal prompt\n",
    "    subliminal_prompt = get_subliminal_prompt(\n",
    "        tokenizer, \n",
    "        number=number, \n",
    "        number_relation=number_relation,\n",
    "        animal_relation=animal_relation,\n",
    "        template_type=template_type\n",
    "    )\n",
    "    \n",
    "    print(f\"Subliminal prompt (number={number}, number_relation={number_relation}, animal_relation={animal_relation}):\")\n",
    "    print(subliminal_prompt)\n",
    "    print()\n",
    "    \n",
    "    # Tokenize and get logits\n",
    "    inputs = tokenizer(subliminal_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    # Get the logits for the last token\n",
    "    last_logits = logits[0, -1, :]\n",
    "    log_probs = torch.log_softmax(last_logits, dim=-1)\n",
    "    \n",
    "    # Get top k most likely next tokens\n",
    "    top_probs, top_indices = torch.topk(log_probs, top_k)\n",
    "    \n",
    "    print(f\"Top {top_k} most likely next tokens:\")\n",
    "    results = []\n",
    "    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):\n",
    "        token = tokenizer.decode(idx)\n",
    "        prob_val = torch.exp(prob).item()\n",
    "        log_prob_val = prob.item()\n",
    "        results.append((token, log_prob_val, prob_val))\n",
    "        print(f\"{i+1:2d}. '{token}' - log_prob: {log_prob_val:.4f} (prob: {prob_val:.4f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test with hating number 864\n",
    "subliminal_hate_results = check_subliminal_top_tokens(\n",
    "    tokenizer, model, \n",
    "    number=\"864\", \n",
    "    number_relation=\"hate\", \n",
    "    animal_relation=\"hate\",\n",
    "    template_type=\"full\",\n",
    "    top_k=50\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evil-owls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
